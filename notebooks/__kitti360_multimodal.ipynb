{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import glob\n",
    "from torch_geometric.data import Data\n",
    "import matplotlib.pyplot as plt\n",
    "from plyfile import PlyData\n",
    "\n",
    "import sys\n",
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "\n",
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data, hex_to_tensor\n",
    "from torch_points3d.core.multimodal.data import MMData\n",
    "from torch_points3d.core.multimodal.image import SameSettingImageData, ImageData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from torch_points3d.utils.config import hydra_read\n",
    "\n",
    "# Set root to the DATA drive, where the data was downloaded\n",
    "# DATA_ROOT = '/mnt/fa444ffd-fdb4-4701-88e7-f00297a8e29b/projects/datasets/kitti360'  # ???\n",
    "# DATA_ROOT = '/media/drobert-admin/DATA/datasets/kitti360'  # IGN DATA\n",
    "# DATA_ROOT = '/media/drobert-admin/DATA2/datasets/kitti360'  # IGN DATA2\n",
    "# DATA_ROOT = '/var/data/drobert/datasets/kitti360'  # AI4GEO\n",
    "# DATA_ROOT = '/home/qt/robertda/scratch/datasets/kitti360'  # CNES\n",
    "DATA_ROOT = '/raid/dataset/pointcloud/data/kitti360'  # ENGIE\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "#     'data=segmentation/kitti360-sparse',\n",
    "    'data=segmentation/multimodal/kitti360-sparse',\n",
    "    'data.mini=True',\n",
    "#     'models=segmentation/sparseconv3d',\n",
    "    'models=segmentation/multimodal/sparseconv3d',\n",
    "#     'model_name=Res16UNet34',\n",
    "    'model_name=Res16UNet34-PointPyramid-early-cityscapes-interpolate',\n",
    "    f\"data.dataroot={os.path.join(DATA_ROOT, '5cm')}\",\n",
    "#     f\"data.dataroot={os.path.join(DATA_ROOT, 'temp')}\",\n",
    "#     '+train_is_trainval=True',\n",
    "    'data.sample_per_epoch=5'\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_points3d.datasets.segmentation.kitti360 import KITTI360Dataset\n",
    "\n",
    "# dataset = KITTI360Dataset(cfg.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.segmentation.multimodal.kitti360 import KITTI360DatasetMM\n",
    "\n",
    "# NB: preprocessing 3D and 2D data takes roughly 1 min per KITTI-360 window \n",
    "dataset = KITTI360DatasetMM(cfg.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.multimodal import MMData, ImageData\n",
    "from torch_points3d.datasets.segmentation.kitti360 import KITTI360_NUM_CLASSES, INV_OBJECT_LABEL, OBJECT_COLOR, CLASS_NAMES, CLASS_COLORS\n",
    "\n",
    "train_2d_transforms = dataset.train_dataset.transform_image.transforms\n",
    "val_2d_transforms = dataset.val_dataset.transform_image.transforms\n",
    "test_2d_transforms = dataset.test_dataset[0].transform_image.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.train_dataset.transform = None\n",
    "dataset.train_dataset.transform_image.transforms = train_2d_transforms[:4]\n",
    "dataset.train_dataset.transform_image.transforms[3].credit = 1408 * 376 * 3\n",
    "\n",
    "dataset.val_dataset.transform = None\n",
    "dataset.val_dataset.transform_image.transforms = val_2d_transforms[:4]\n",
    "dataset.val_dataset.transform_image.transforms[3].credit = 1408 * 376 * 3\n",
    "\n",
    "dataset.test_dataset[0].transform = dataset.val_dataset.transform\n",
    "dataset.test_dataset[0].transform_image = dataset.val_dataset.transform_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sample\n",
    "# mm_data = dataset.train_dataset[dataset.train_dataset._pick_random_label_and_window()]\n",
    "mm_data = dataset.val_dataset[50]\n",
    "# mm_data = dataset.test_dataset[0][44]\n",
    "\n",
    "# Val sample\n",
    "# mm_data = dataset.val_dataset[np.random.randint(len(dataset.val_dataset[0]))]\n",
    "\n",
    "visualize_mm_data(mm_data, figsize=1000, pointsize=3, voxel=0.5, show_2d=True, front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen = 0\n",
    "samples = 1000\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(samples)):\n",
    "    mm_data = dataset.test_dataset[0][i]\n",
    "    if mm_data.modalities['image'].num_views < 1:\n",
    "#         print(i)\n",
    "        unseen += 1\n",
    "\n",
    "print(unseen / samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen = 0\n",
    "samples = 1000\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(samples)):\n",
    "    mm_data = dataset.val_dataset[i]\n",
    "    if mm_data.modalities['image'].num_views < 1:\n",
    "#         print(i)\n",
    "        unseen += 1\n",
    "\n",
    "print(unseen / samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize a large sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.segmentation.kitti360_config import CLASS_NAMES, CLASS_COLORS\n",
    "from torch_points3d.core.multimodal.data import MMData\n",
    "from torch_points3d.core.multimodal.image import ImageData\n",
    "\n",
    "dataset.val_dataset[0]\n",
    "mm_window = dataset.val_dataset.buffer[0]\n",
    "\n",
    "# dataset.test_dataset[0][0]\n",
    "# mm_window = dataset.test_dataset[0].buffer[0]\n",
    "\n",
    "mm_data_large = MMData(mm_window.data, image=ImageData(mm_window.images))\n",
    "\n",
    "visualize_mm_data(mm_data_large, figsize=1000, pointsize=3, voxel=1, show_2d=False, front='map', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=2, max_points=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.segmentation.kitti360_config import CLASS_NAMES, CLASS_COLORS\n",
    "from torch_points3d.core.multimodal.data import MMData\n",
    "from torch_points3d.core.multimodal.image import ImageData\n",
    "\n",
    "# dataset.val_dataset[0]\n",
    "# mm_window = dataset.val_dataset.buffer[0]\n",
    "\n",
    "dataset.test_dataset[0][1]\n",
    "mm_window = dataset.test_dataset[0].buffer[1]\n",
    "\n",
    "mm_data_large = MMData(mm_window.data, image=ImageData(mm_window.images))\n",
    "\n",
    "visualize_mm_data(mm_data_large, figsize=1000, pointsize=3, voxel=1, show_2d=False, front='map', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=2, max_points=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_points3d.core.data_transform as cT\n",
    "from torch_points3d.core.data_transform.multimodal.image import SelectMappingFromPointId, PickImagesFromMappingArea\n",
    "from torch_points3d.core.multimodal.data import MMData, MMBatch\n",
    "from torch_points3d.core.multimodal.image import ImageData, SameSettingImageData\n",
    "\n",
    "# Sample sphere at chosen locatiopn\n",
    "opacity = 0.4\n",
    "radius = 6\n",
    "center = torch.Tensor([1012, 3843, 114])\n",
    "\n",
    "# Take ball, take images, make into a MMData\n",
    "data_small = cT.CylinderSampling(radius, center[:2], align_origin=False)(mm_window.data).clone()\n",
    "data_small = Data(pos=data_small.pos, rgb=data_small.rgb, y=data_small.y, mapping_index=data_small.mapping_index, origin_id=data_small.origin_id)\n",
    "data_small, images = SelectMappingFromPointId()(data_small, mm_window.images)\n",
    "data_small, images = PickImagesFromMappingArea(area_ratio=0.02, use_bbox=True)(data_small, images)\n",
    "# data_small, images = PickImagesFromMemoryCredit(img_size=images.ref_size, n_img=8)(data_small, images)\n",
    "data_small = MMData(data_small, image=ImageData([images]))\n",
    "\n",
    "# Take the surroundings, remove the center, remove image mappings, convert to MMData\n",
    "data_large = cT.SphereSampling(radius * 3, center[:2], align_origin=False)(mm_window.data).clone()\n",
    "is_in_small = torch.from_numpy(np.isin(data_large.origin_id.numpy(), data_small.origin_id.numpy()))\n",
    "data_large = Data(pos=data_large.pos[~is_in_small], rgb=data_large.rgb[~is_in_small], y=data_large.y[~is_in_small], mapping_index=data_large.mapping_index[~is_in_small])\n",
    "data_large.mapping_index = torch.arange(data_large.num_nodes)\n",
    "data_large.rgb = 1 - opacity * (1 - data_large.rgb)\n",
    "empty_images = images[0][[]]\n",
    "empty_images.mappings.pointers = torch.zeros(data_large.num_nodes + 1, dtype=torch.long)\n",
    "data_large = MMData(data_large, image=ImageData([empty_images]))\n",
    "\n",
    "# Combine both into a MMBatch\n",
    "mm_data = MMBatch.from_mm_data_list([data_large, data_small])\n",
    "\n",
    "# Remove the ceiling\n",
    "# mm_data = mm_data[mm_data.data.pos[:, 2] < 2.5]\n",
    "# mm_data = mm_data[mm_data.data.pos[:, 0] < 3.3]\n",
    "\n",
    "#-----------\n",
    "# data, images = SelectMappingFromPointId()(data_large, dataset.test_dataset[0]._images[0])\n",
    "\n",
    "# # Reduce the number of images\n",
    "# data, images = PickImagesFromMappingArea(area_ratio=0.02, use_bbox=True)(data, images)\n",
    "# data, images = PickImagesFromMemoryCredit(img_size=[512, 256], n_img=6)(data, images)\n",
    "\n",
    "# # Convert to MMData\n",
    "# mm_data = MMData(data, image=images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mm_data(mm_data, class_names=CLASS_NAMES, class_colors=CLASS_COLORS, figsize=1600, voxel=0.05, show_2d=False, pointsize=3, front=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.models.model_factory import instantiate_model\n",
    "\n",
    "model = instantiate_model(cfg, dataset)\n",
    "# model = model.eval().cuda()\n",
    "# model = model.train().cpu()\n",
    "model = model.train().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.multimodal import MMData, MMBatch\n",
    "\n",
    "mm_data = MMBatch.from_mm_data_list([dataset.val_dataset[2893*4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.multimodal import MMData, MMBatch\n",
    "\n",
    "mm_data = MMBatch.from_mm_data_list([dataset.val_dataset[4], dataset.val_dataset[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_input(mm_data, model.device)\n",
    "\n",
    "_ = model(mm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss_seg.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_input(mm_data, model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(mm_data)\n",
    "model.output.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.input = {\n",
    "                'x_3d': sp3d.nn.SparseTensor(data.x, data.coords, data.batch, self.device),\n",
    "                'x_seen': None,\n",
    "                'modalities': data.to(self.device).modalities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_points3d.modules.SparseConv3d as sp3d\n",
    "\n",
    "in_data = mm_data.clone()\n",
    "\n",
    "mm_data_dict = {\n",
    "    'x_3d': sp3d.nn.SparseTensor(in_data.data.x, in_data.data.coords, in_data.data.batch, model.device),\n",
    "    'x_seen': None,\n",
    "    'modalities': in_data.to(model.device).modalities }\n",
    "\n",
    "out_dict = model.backbone.down_modules[0](mm_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data.modalities['image'].x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict['modalities']['image'].x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data = dataset.val_dataset[2893*4 + 0]\n",
    "print(mm_data)\n",
    "\n",
    "images = mm_data.modalities['image'][0]\n",
    "# images = images.select_views([])\n",
    "print(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images[[]]\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images.select_points([])\n",
    "idx = \n",
    "images.mappings.pointers = torch.zeros(idx.shape[0] + 1).long().to(images.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.select_points(torch.LongTensor(()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.segmentation.kitti360 import read_kitti360_window\n",
    "from tqdm import tqdm\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    print(split)\n",
    "    \n",
    "    dataset_stage = dataset.get_dataset(split)\n",
    "    \n",
    "    for i, (path, sampling_path, raw_path) in tqdm(enumerate(zip(dataset_stage.paths, dataset_stage.sampling_paths, dataset_stage.raw_3d_paths))):\n",
    "        \n",
    "        if not osp.splitext(osp.basename(path))[0][:21] == osp.splitext(osp.basename(sampling_path))[0][:21] == osp.splitext(osp.basename(raw_path))[0][:21]:\n",
    "            print(split, i, 'has name issues')\n",
    "        \n",
    "        sampling = torch.load(sampling_path)\n",
    "        \n",
    "        num_points = torch.load(path).num_nodes\n",
    "        if num_points != sampling['num_points']:\n",
    "            print(split, i, 'has num_points issues')\n",
    "            sampling['num_points'] = num_points\n",
    "#             torch.save(sampling, sampling_path)\n",
    "\n",
    "        num_raw_points = read_kitti360_window(raw_path, xyz=True, rgb=False, semantic=False, instance=False).num_nodes\n",
    "        if num_raw_points != sampling['num_raw_points']:\n",
    "            print(split, i, 'has num_raw_points issues')\n",
    "            sampling['num_raw_points'] = num_raw_points\n",
    "#             torch.save(sampling, sampling_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data.modalities['image'].select_views([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.val_dataset[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.multimodal.data import MMBatch\n",
    "\n",
    "mm_data = dataset.val_dataset[2893*4 + 0]\n",
    "mm_data.modalities['image'] = mm_data.modalities['image'].select_views([])\n",
    "mm_data = MMBatch.from_mm_data_list([mm_data, mm_data, mm_data])\n",
    "\n",
    "# mm_data = MMBatch.from_mm_data_list([dataset.val_dataset[2893*4 + 0]])\n",
    "\n",
    "model.set_input(mm_data, model.device)\n",
    "out = model.forward(mm_data)\n",
    "model.output.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone.down_modules[3].image.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data.to_mm_data_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.multimodal import MMData, ImageData\n",
    "from torch_points3d.datasets.segmentation.kitti360 import KITTI360_NUM_CLASSES, INV_OBJECT_LABEL, OBJECT_COLOR, CLASS_NAMES, CLASS_COLORS\n",
    "\n",
    "dataset.train_dataset.transform = None\n",
    "dataset.train_dataset.transform_image.transforms = dataset.train_dataset.transform_image.transforms[:4]\n",
    "dataset.train_dataset.transform_image.transforms[3].credit = 1408 * 376 * 3\n",
    "\n",
    "dataset.val_dataset.transform = None\n",
    "dataset.val_dataset.transform_image.transforms = dataset.val_dataset.transform_image.transforms[:4]\n",
    "dataset.val_dataset.transform_image.transforms[3].credit = 1408 * 376 * 3\n",
    "\n",
    "# dataset.test_dataset[0].transform = dataset.val_dataset.transform\n",
    "# dataset.test_dataset[0].transform_image = dataset.val_dataset.transform_image\n",
    "\n",
    "# Train sample\n",
    "mm_data = dataset.train_dataset[dataset.train_dataset._pick_random_label_and_window()]\n",
    "\n",
    "# Val sample\n",
    "# mm_data = dataset.val_dataset[np.random.randint(len(dataset.val_dataset[0]))]\n",
    "\n",
    "visualize_mm_data(mm_data, figsize=1000, pointsize=3, voxel=0.5, show_2d=True, front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '/raid/dataset/pointcloud/data/kitti_360/KITTI-360'\n",
    "raw_2d_dir = osp.join(DATA_ROOT, 'data_2d_raw')\n",
    "\n",
    "scan_names = sorted(os.listdir(raw_2d_dir))\n",
    "print(f'There are {len(scan_names)} scans')\n",
    "for scan_name in scan_names:\n",
    "    print(f'  {scan_name}')\n",
    "\n",
    "camera_names = ['image_00', 'image_01']\n",
    "\n",
    "images = sorted(glob.glob(osp.join(raw_2d_dir, scan_names[0], camera_names[0], 'data_rect', '*.png')))\n",
    "image_names = sorted(os.listdir(osp.join(raw_2d_dir, scan_names[0], camera_names[0], 'data_rect')))\n",
    "\n",
    "print(f\"\\nThere are 2 x {len(image_names)} perspective images in '{scan_names[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for i in range(200, 260, 20):\n",
    "    image_path_0 = osp.join(raw_2d_dir, scan_names[0], camera_names[0], 'data_rect', image_names[i])\n",
    "    image_path_1 = osp.join(raw_2d_dir, scan_names[0], camera_names[1], 'data_rect', image_names[i])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(24, 4))\n",
    "    \n",
    "    axes[0].imshow(np.asarray(Image.open(image_path_0)))\n",
    "    axes[1].imshow(np.asarray(Image.open(image_path_1)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the perspective cameras are redundant, they see roughly the same thing. So we will only use one for our application.Î¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readVariable(fid,name,M,N):\n",
    "    # rewind\n",
    "    fid.seek(0,0)\n",
    "    \n",
    "    # search for variable identifier\n",
    "    line = 1\n",
    "    success = 0\n",
    "    while line:\n",
    "        line = fid.readline()\n",
    "        if line.startswith(name):\n",
    "            success = 1\n",
    "            break\n",
    "\n",
    "    # return if variable identifier not found\n",
    "    if success==0:\n",
    "        return None\n",
    "    \n",
    "    # fill matrix\n",
    "    line = line.replace('%s:' % name, '')\n",
    "    line = line.split()\n",
    "    assert(len(line) == M*N)\n",
    "    line = [float(x) for x in line]\n",
    "    mat = np.array(line).reshape(M, N)\n",
    "\n",
    "    return mat\n",
    "\n",
    "def checkfile(filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        raise RuntimeError('%s does not exist!' % filename)\n",
    "        \n",
    "def loadCalibrationCameraToPose(filename):\n",
    "    # check file\n",
    "    checkfile(filename)\n",
    "\n",
    "    # open file\n",
    "    fid = open(filename,'r');\n",
    "     \n",
    "    # read variables\n",
    "    Tr = {}\n",
    "    cameras = ['image_00', 'image_01', 'image_02', 'image_03']\n",
    "    lastrow = np.array([0,0,0,1]).reshape(1,4)\n",
    "    for camera in cameras:\n",
    "        Tr[camera] = np.concatenate((readVariable(fid, camera, 3, 4), lastrow))\n",
    "      \n",
    "    # close file\n",
    "    fid.close()\n",
    "    return Tr\n",
    "\n",
    "class Camera:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # load intrinsics\n",
    "        self.load_intrinsics(self.intrinsic_file)\n",
    "\n",
    "        # load poses\n",
    "        poses = np.loadtxt(self.pose_file)\n",
    "        frames = poses[:,0].astype(np.int64)\n",
    "        poses = np.reshape(poses[:,1:],[-1,3,4])\n",
    "        self.cam2world = {}\n",
    "        self.frames = frames\n",
    "        for frame, pose in zip(frames, poses): \n",
    "            pose = np.concatenate((pose, np.array([0.,0.,0.,1.]).reshape(1,4)))\n",
    "            # consider the rectification for perspective cameras\n",
    "            if self.cam_id==0 or self.cam_id==1:\n",
    "                self.cam2world[frame] = np.matmul(np.matmul(pose, self.camToPose),\n",
    "                                                  np.linalg.inv(self.R_rect))\n",
    "            # fisheye cameras\n",
    "            elif self.cam_id==2 or self.cam_id==3:\n",
    "                self.cam2world[frame] = np.matmul(pose, self.camToPose)\n",
    "            else:\n",
    "                raise RuntimeError('Unknown Camera ID!')\n",
    "\n",
    "\n",
    "    def world2cam(self, points, R, T, inverse=False):\n",
    "        assert (points.ndim==R.ndim)\n",
    "        assert (T.ndim==R.ndim or T.ndim==(R.ndim-1)) \n",
    "        ndim=R.ndim\n",
    "        if ndim==2:\n",
    "            R = np.expand_dims(R, 0) \n",
    "            T = np.reshape(T, [1, -1, 3])\n",
    "            points = np.expand_dims(points, 0)\n",
    "        if not inverse:\n",
    "            points = np.matmul(R, points.transpose(0,2,1)).transpose(0,2,1) + T\n",
    "        else:\n",
    "            points = np.matmul(R.transpose(0,2,1), (points - T).transpose(0,2,1))\n",
    "\n",
    "        if ndim==2:\n",
    "            points = points[0]\n",
    "\n",
    "        return points\n",
    "\n",
    "    def cam2image(self, points):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_intrinsics(self, intrinsic_file):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def project_vertices(self, vertices, frameId, inverse=True):\n",
    "\n",
    "        # current camera pose\n",
    "        curr_pose = self.cam2world[frameId]\n",
    "        T = curr_pose[:3,  3]\n",
    "        R = curr_pose[:3, :3]\n",
    "\n",
    "        # convert points from world coordinate to local coordinate \n",
    "        points_local = self.world2cam(vertices, R, T, inverse)\n",
    "\n",
    "        # perspective projection\n",
    "        u,v,depth = self.cam2image(points_local)\n",
    "\n",
    "        return (u,v), depth \n",
    "\n",
    "    def __call__(self, obj3d, frameId):\n",
    "\n",
    "        vertices = obj3d.vertices\n",
    "\n",
    "        uv, depth = self.project_vertices(vertices, frameId)\n",
    "\n",
    "        obj3d.vertices_proj = uv\n",
    "        obj3d.vertices_depth = depth \n",
    "        obj3d.generateMeshes()\n",
    "\n",
    "\n",
    "class CameraPerspective(Camera):\n",
    "\n",
    "    def __init__(self, root_dir, seq='2013_05_28_drive_0009_sync', cam_id=0):\n",
    "        # perspective camera ids: {0,1}, fisheye camera ids: {2,3}\n",
    "        assert (cam_id==0 or cam_id==1)\n",
    "\n",
    "        pose_dir = os.path.join(root_dir, 'data_poses', seq)\n",
    "        calib_dir = os.path.join(root_dir, 'calibration')\n",
    "        self.pose_file = os.path.join(pose_dir, \"poses.txt\")\n",
    "        self.intrinsic_file = os.path.join(calib_dir, 'perspective.txt')\n",
    "        fileCameraToPose = os.path.join(calib_dir, 'calib_cam_to_pose.txt')\n",
    "        self.camToPose = loadCalibrationCameraToPose(fileCameraToPose)['image_%02d' % cam_id]\n",
    "        self.cam_id = cam_id\n",
    "        super(CameraPerspective, self).__init__()\n",
    "\n",
    "    def load_intrinsics(self, intrinsic_file):\n",
    "        ''' load perspective intrinsics '''\n",
    "    \n",
    "        intrinsic_loaded = False\n",
    "        width = -1\n",
    "        height = -1\n",
    "        with open(intrinsic_file) as f:\n",
    "            intrinsics = f.read().splitlines()\n",
    "        for line in intrinsics:\n",
    "            line = line.split(' ')\n",
    "            if line[0] == 'P_rect_%02d:' % self.cam_id:\n",
    "                K = [float(x) for x in line[1:]]\n",
    "                K = np.reshape(K, [3,4])\n",
    "                intrinsic_loaded = True\n",
    "            elif line[0] == 'R_rect_%02d:' % self.cam_id:\n",
    "                R_rect = np.eye(4) \n",
    "                R_rect[:3,:3] = np.array([float(x) for x in line[1:]]).reshape(3,3)\n",
    "            elif line[0] == \"S_rect_%02d:\" % self.cam_id:\n",
    "                width = int(float(line[1]))\n",
    "                height = int(float(line[2]))\n",
    "        assert(intrinsic_loaded==True)\n",
    "        assert(width>0 and height>0)\n",
    "    \n",
    "        self.K = K\n",
    "        self.width, self.height = width, height\n",
    "        self.R_rect = R_rect\n",
    "\n",
    "    def cam2image(self, points):\n",
    "        ndim = points.ndim\n",
    "        if ndim == 2:\n",
    "            points = np.expand_dims(points, 0)\n",
    "        points_proj = np.matmul(self.K[:3,:3].reshape([1,3,3]), points)\n",
    "        depth = points_proj[:,2,:]\n",
    "        depth[depth==0] = -1e-6\n",
    "        u = np.round(points_proj[:,0,:]/np.abs(depth)).astype(np.int64)\n",
    "        v = np.round(points_proj[:,1,:]/np.abs(depth)).astype(np.int64)\n",
    "\n",
    "        if ndim==2:\n",
    "            u = u[0]; v=v[0]; depth=depth[0]\n",
    "        return u, v, depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CameraPerspective(DATA_ROOT, seq='2013_05_28_drive_0000_sync', cam_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uv, d = camera.project_vertices(np.eye(3), 1)\n",
    "uv, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_intrinsics(intrinsic_file, cam_id=0):\n",
    "    ''' load KITTIU360 perspective camera intrinsics \n",
    "    \n",
    "    Credit: https://github.com/autonomousvision/kitti360Scripts\n",
    "    '''\n",
    "\n",
    "    intrinsic_loaded = False\n",
    "    width = -1\n",
    "    height = -1\n",
    "    with open(intrinsic_file) as f:\n",
    "        intrinsics = f.read().splitlines()\n",
    "    for line in intrinsics:\n",
    "        line = line.split(' ')\n",
    "        if line[0] == f'P_rect_0{cam_id}:':\n",
    "            K = [float(x) for x in line[1:]]\n",
    "            K = np.reshape(K, [3,4])\n",
    "            intrinsic_loaded = True\n",
    "        elif line[0] == f'R_rect_0{cam_id}:':\n",
    "            R_rect = np.eye(4) \n",
    "            R_rect[:3,:3] = np.array([float(x) for x in line[1:]]).reshape(3,3)\n",
    "        elif line[0] == f\"S_rect_0{cam_id}:\":\n",
    "            width = int(float(line[1]))\n",
    "            height = int(float(line[2]))\n",
    "    assert(intrinsic_loaded==True)\n",
    "    assert(width>0 and height>0)\n",
    "    \n",
    "    return K, R_rect, width, height\n",
    "        \n",
    "        \n",
    "def loadCalibrationCameraToPose(filename):\n",
    "    ''' load KITTIU360 camera-to-pose calibration \n",
    "    \n",
    "    Credit: https://github.com/autonomousvision/kitti360Scripts\n",
    "    '''\n",
    "    Tr = {}\n",
    "    with open(filename,'r') as fid:\n",
    "        cameras = ['image_00', 'image_01', 'image_02', 'image_03']\n",
    "        lastrow = np.array([0,0,0,1]).reshape(1,4)\n",
    "        for camera in cameras:\n",
    "            Tr[camera] = np.concatenate((readVariable(fid, camera, 3, 4), lastrow))\n",
    "    return Tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = scan_names[0]\n",
    "cam_id = 0\n",
    "\n",
    "pose_dir = osp.join(DATA_ROOT, 'data_poses', seq)\n",
    "calib_dir = osp.join(DATA_ROOT, 'calibration')\n",
    "\n",
    "pose_file = osp.join(pose_dir, \"poses.txt\")\n",
    "intrinsic_file = osp.join(calib_dir, 'perspective.txt')\n",
    "fileCameraToPose = osp.join(calib_dir, 'calib_cam_to_pose.txt')\n",
    "camToPose = loadCalibrationCameraToPose(fileCameraToPose)[f'image_0{cam_id}']\n",
    "\n",
    "K, R_rect, width, height = load_intrinsics(intrinsic_file, cam_id=cam_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE = -1\n",
    "\n",
    "# Credit: https://github.com/autonomousvision/kitti360Scripts/blob/master/kitti360scripts/helpers/labels.py\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "labels = [\n",
    "    #       name                     id    kittiId,    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,       -1 ,    IGNORE , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,       -1 ,    IGNORE , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,       -1 ,    IGNORE , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,       -1 ,    IGNORE , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,       -1 ,    IGNORE , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,       -1 ,    IGNORE , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,       -1 ,    IGNORE , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        1 ,         0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        3 ,         1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,        2 ,    IGNORE , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,        10,    IGNORE , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        11,         2 , 'construction'    , 2       , True         , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        7 ,         3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        8 ,         4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,        30,    IGNORE , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,        31,    IGNORE , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,        32,    IGNORE , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        21,         5 , 'object'          , 3       , True         , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,       -1 ,    IGNORE , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        23,         6 , 'object'          , 3       , True         , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        24,         7 , 'object'          , 3       , True         , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        5 ,         8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        4 ,         9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,        9 ,        10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,        19,        11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,        20,        12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,        13,        13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,        14,        14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,        34,        15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,        16,    IGNORE , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,        15,    IGNORE , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,        33,        16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,        17,        17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,        18,        18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'garage'               , 34 ,        12,         2 , 'construction'    , 2       , True         , False        , ( 64,128,128) ),\n",
    "    Label(  'gate'                 , 35 ,        6 ,         4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'stop'                 , 36 ,        29,    IGNORE , 'construction'    , 2       , True         , True         , (150,120, 90) ),\n",
    "    Label(  'smallpole'            , 37 ,        22,         5 , 'object'          , 3       , True         , False        , (153,153,153) ),\n",
    "    Label(  'lamp'                 , 38 ,        25,    IGNORE , 'object'          , 3       , True         , False        , (0,   64, 64) ),\n",
    "    Label(  'trash bin'            , 39 ,        26,    IGNORE , 'object'          , 3       , True         , False        , (0,  128,192) ),\n",
    "    Label(  'vending machine'      , 40 ,        27,    IGNORE , 'object'          , 3       , True         , False        , (128, 64,  0) ),\n",
    "    Label(  'box'                  , 41 ,        28,    IGNORE , 'object'          , 3       , True         , False        , (64,  64,128) ),\n",
    "    Label(  'unknown construction' , 42 ,        35,    IGNORE , 'void'            , 0       , False        , True         , (102,  0,  0) ),\n",
    "    Label(  'unknown vehicle'      , 43 ,        36,    IGNORE , 'void'            , 0       , False        , True         , ( 51,  0, 51) ),\n",
    "    Label(  'unknown object'       , 44 ,        37,    IGNORE , 'void'            , 0       , False        , True         , ( 32, 32, 32) ),\n",
    "    Label(  'license plate'        , -1 ,        -1,        -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
    "]\n",
    "\n",
    "# Dictionaries for a fast lookup\n",
    "NAME2LABEL = {label.name: label for label in labels}\n",
    "ID2LABEL = {label.id: label for label in labels}\n",
    "TRAINID2LABEL = {label.trainId: label for label in reversed(labels)}\n",
    "KITTIID2LABEL = {label.kittiId: label for label in labels}  # KITTI-360 ID to cityscapes ID\n",
    "CATEGORY2LABELS = {}\n",
    "for label in labels:\n",
    "    category = label.category\n",
    "    if category in CATEGORY2LABELS:\n",
    "        CATEGORY2LABELS[category].append(label)\n",
    "    else:\n",
    "        CATEGORY2LABELS[category] = [label]\n",
    "\n",
    "CLASS_NAMES = [ID2LABEL[x].name for x in range(45)]\n",
    "CLASS_COLORS = [ID2LABEL[x].color for x in range(45)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    #       name                     id    kittiId,    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,       -1 ,    IGNORE , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,       -1 ,    IGNORE , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,       -1 ,    IGNORE , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,       -1 ,    IGNORE , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,       -1 ,    IGNORE , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,       -1 ,    IGNORE , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,       -1 ,    IGNORE , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        1 ,         0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        3 ,         1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,        2 ,    IGNORE , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,        10,    IGNORE , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        11,         2 , 'construction'    , 2       , True         , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        7 ,         3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        8 ,         4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,        30,    IGNORE , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,        31,    IGNORE , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,        32,    IGNORE , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        21,         5 , 'object'          , 3       , True         , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,       -1 ,    IGNORE , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        23,         6 , 'object'          , 3       , True         , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        24,         7 , 'object'          , 3       , True         , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        5 ,         8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        4 ,         9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,        9 ,        10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,        19,        11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,        20,        12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,        13,        13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,        14,        14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,        34,        15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,        16,    IGNORE , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,        15,    IGNORE , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,        33,        16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,        17,        17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,        18,        18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'garage'               , 34 ,        12,         2 , 'construction'    , 2       , True         , False        , ( 64,128,128) ),\n",
    "    Label(  'gate'                 , 35 ,        6 ,         4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'stop'                 , 36 ,        29,    IGNORE , 'construction'    , 2       , True         , True         , (150,120, 90) ),\n",
    "    Label(  'smallpole'            , 37 ,        22,         5 , 'object'          , 3       , True         , False        , (153,153,153) ),\n",
    "    Label(  'lamp'                 , 38 ,        25,    IGNORE , 'object'          , 3       , True         , False        , (0,   64, 64) ),\n",
    "    Label(  'trash bin'            , 39 ,        26,    IGNORE , 'object'          , 3       , True         , False        , (0,  128,192) ),\n",
    "    Label(  'vending machine'      , 40 ,        27,    IGNORE , 'object'          , 3       , True         , False        , (128, 64,  0) ),\n",
    "    Label(  'box'                  , 41 ,        28,    IGNORE , 'object'          , 3       , True         , False        , (64,  64,128) ),\n",
    "    Label(  'unknown construction' , 42 ,        35,    IGNORE , 'void'            , 0       , False        , True         , (102,  0,  0) ),\n",
    "    Label(  'unknown vehicle'      , 43 ,        36,    IGNORE , 'void'            , 0       , False        , True         , ( 51,  0, 51) ),\n",
    "    Label(  'unknown object'       , 44 ,        37,    IGNORE , 'void'            , 0       , False        , True         , ( 32, 32, 32) ),\n",
    "    Label(  'license plate'        , -1 ,        -1,        -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
    "]\n",
    "\n",
    "# Dictionaries for a fast lookup\n",
    "KITTI360_NUM_CLASSES = 19\n",
    "NAME2LABEL = {label.name: label for label in labels}\n",
    "ID2LABEL = {label.id: label for label in labels}\n",
    "TRAINID2LABEL = {label.trainId: label for label in reversed(labels)}\n",
    "KITTIID2LABEL = {label.kittiId: label for label in labels}  # KITTI-360 ID to cityscapes ID\n",
    "CATEGORY2LABELS = {}\n",
    "for label in labels:\n",
    "    category = label.category\n",
    "    if category in CATEGORY2LABELS:\n",
    "        CATEGORY2LABELS[category].append(label)\n",
    "    else:\n",
    "        CATEGORY2LABELS[category] = [label]\n",
    "INV_OBJECT_LABEL = {k: TRAINID2LABEL[k].name for k in range(KITTI360_NUM_CLASSES)}\n",
    "OBJECT_COLOR = np.asarray([TRAINID2LABEL[k].color for k in range(KITTI360_NUM_CLASSES)])\n",
    "OBJECT_LABEL = {name: i for i, name in INV_OBJECT_LABEL.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TRAINID2LABEL)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INV_OBJECT_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID2TRAINID = [label.trainId for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID2TRAINID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[label.id for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: TRAINID2LABEL[k].name for k in sorted(TRAINID2LABEL.keys())[1:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KITTI360_NUM_CLASSES = 19\n",
    "INV_OBJECT_LABEL = {k: TRAINID2LABEL[k].name for k in range(KITTI360_NUM_CLASSES)}\n",
    "OBJECT_COLOR = np.asarray([TRAINID2LABEL[k].color for k in range(KITTI360_NUM_CLASSES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{name: i for i, name in INV_OBJECT_LABEL.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '/raid/dataset/pointcloud/data/kitti_360/KITTI-360'\n",
    "\n",
    "for raw_3d_dir in [osp.join(DATA_ROOT, 'data_3d_semantics'), osp.join(DATA_ROOT, 'data_3d_semantics_test')]:\n",
    "    scan_names = sorted(os.listdir(raw_3d_dir))\n",
    "    print(f'\\nThere are {len(scan_names)} {osp.basename(raw_3d_dir)} scans')\n",
    "    for scan_name in scan_names:\n",
    "        window_names = os.listdir(osp.join(raw_3d_dir, scan_name, \"static\"))\n",
    "        num_points = []\n",
    "        for window_name in window_names:\n",
    "            with open(osp.join(raw_3d_dir, scan_name, 'static', window_name), \"rb\") as f:\n",
    "                data = PlyData.read(f)\n",
    "                num_points.append(torch.tensor(data[\"vertex\"][\"red\"]).shape[0])\n",
    "        print(f'  {scan_name} - {len(window_names):>2} windows - total={np.sum(num_points) / 10**6:0.1f} - mean={np.mean(num_points) / 10**6:0.1f} - min={np.min(num_points) / 10**6:0.1f} - max={np.max(num_points) / 10**6:0.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunk(filepath, instance=False):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        chunk = PlyData.read(f)\n",
    "\n",
    "        pos = torch.stack([torch.tensor(chunk[\"vertex\"][axis]) for axis in [\"x\", \"y\", \"z\"]], dim=-1)\n",
    "\n",
    "        rgb = torch.stack([torch.tensor(chunk[\"vertex\"][axis]) for axis in [\"red\", \"green\", \"blue\"]], dim=-1).float() / 255\n",
    "\n",
    "        y = torch.tensor(chunk[\"vertex\"]['semantic'])\n",
    "        \n",
    "        data = Data(pos=pos, rgb=rgb, y=y, mapping_index=torch.arange(pos.shape[0]))\n",
    "        \n",
    "        if instance:\n",
    "            data.instance = torch.tensor(chunk[\"vertex\"]['instance'])\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '/raid/dataset/pointcloud/data/kitti_360/KITTI-360'\n",
    "raw_3d_dir = osp.join(DATA_ROOT, 'data_3d_semantics')\n",
    "scan_name = sorted(os.listdir(raw_3d_dir))[0]\n",
    "window_name = os.listdir(osp.join(raw_3d_dir, scan_name, \"static\"))[0]\n",
    "\n",
    "mm_data = MMData(data=read_chunk(osp.join(raw_3d_dir, scan_name, 'static', window_name)), image=ImageData([SameSettingImageData()]))\n",
    "\n",
    "print(f'Raw cloud size: {mm_data.data.num_nodes / 10**6:0.3f} M points')\n",
    "\n",
    "from torch_points3d.core.data_transform.grid_transform import GridSampling3D\n",
    "\n",
    "print('\\nSubsampling the cloud...')\n",
    "for voxel in [1.0, 0.5, 0.25, 0.1, 0.08, 0.05, 0.01]:\n",
    "    sub_cloud = GridSampling3D(voxel, mode='last')(mm_data.data.clone())\n",
    "    print(f'voxel={voxel:>4}, n={sub_cloud.num_nodes:>7}, ratio={sub_cloud.num_nodes / mm_data.data.num_nodes * 100:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data = MMData(data=read_chunk(osp.join(raw_3d_dir, scan_name, 'static', window_name)), image=ImageData([SameSettingImageData()]))\n",
    "\n",
    "visualize_mm_data(mm_data, figsize=1000, pointsize=3, voxel=0.5, show_2d=False, class_names=class_names, class_colors=class_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to visualize image centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = scan_name\n",
    "cam_id = 0\n",
    "\n",
    "pose_dir = osp.join(DATA_ROOT, 'data_poses', seq)\n",
    "calib_dir = osp.join(DATA_ROOT, 'calibration')\n",
    "\n",
    "pose_file = osp.join(pose_dir, \"poses.txt\")\n",
    "\n",
    "poses = np.loadtxt(pose_file)[:350:20]\n",
    "frames = poses[:, 0].astype(np.int64)\n",
    "poses = poses[:, 1:].reshape(-1, 3, 4)\n",
    "\n",
    "n_images = poses.shape[0]\n",
    "extrinsic = torch.from_numpy(poses)\n",
    "xyz = extrinsic[:, :3, 3]\n",
    "fx = torch.zeros(n_images)\n",
    "fy = torch.zeros(n_images)\n",
    "mx = torch.zeros(n_images)\n",
    "my = torch.zeros(n_images)\n",
    "\n",
    "image_data = SameSettingImageData(ref_size=(width, height), proj_upscale=1, path=np.zeros(n_images).astype('O'), pos=xyz, fx=fx, fy=fy, mx=mx, my=my, extrinsic=extrinsic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data = MMData(data=read_chunk(osp.join(raw_3d_dir, scan_names[0], 'static', scan_ply_chunks[0])), image=ImageData(image_data))\n",
    "\n",
    "visualize_mm_data(mm_data, figsize=1000, pointsize=3, voxel=0.5, show_2d=False, class_names=class_names, class_colors=class_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for i_frame, frame in enumerate(frames):\n",
    "    image_path_0 = osp.join(raw_2d_dir, scan_names[0], camera_names[0], 'data_rect', f'{frame:010d}.png')\n",
    "    image_path_1 = osp.join(raw_2d_dir, scan_names[0], camera_names[1], 'data_rect', f'{frame:010d}.png')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(24, 4))\n",
    "    \n",
    "    axes[0].imshow(np.asarray(Image.open(image_path_0)))\n",
    "    axes[1].imshow(np.asarray(Image.open(image_path_1)))\n",
    "    fig.suptitle(f'{i_frame}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KITTI360 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunk(filepath, instance=False):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        chunk = PlyData.read(f)\n",
    "\n",
    "        pos = torch.stack([torch.FloatTensor(chunk[\"vertex\"][axis]) for axis in [\"x\", \"y\", \"z\"]], dim=-1)\n",
    "\n",
    "        rgb = torch.stack([torch.FloatTensor(chunk[\"vertex\"][axis]) for axis in [\"red\", \"green\", \"blue\"]], dim=-1) / 255\n",
    "\n",
    "        y = torch.LongTensor(chunk[\"vertex\"]['semantic'])\n",
    "        \n",
    "        data = Data(pos=pos, rgb=rgb, y=y, mapping_index=torch.arange(pos.shape[0]))\n",
    "        \n",
    "        if instance:\n",
    "            data.instance = torch.LongTensor(chunk[\"vertex\"]['instance'])\n",
    "        \n",
    "        return data\n",
    "\n",
    "def readVariable(fid,name,M,N):\n",
    "    # rewind\n",
    "    fid.seek(0,0)\n",
    "    \n",
    "    # search for variable identifier\n",
    "    line = 1\n",
    "    success = 0\n",
    "    while line:\n",
    "        line = fid.readline()\n",
    "        if line.startswith(name):\n",
    "            success = 1\n",
    "            break\n",
    "\n",
    "    # return if variable identifier not found\n",
    "    if success==0:\n",
    "        return None\n",
    "    \n",
    "    # fill matrix\n",
    "    line = line.replace('%s:' % name, '')\n",
    "    line = line.split()\n",
    "    assert(len(line) == M*N)\n",
    "    line = [float(x) for x in line]\n",
    "    mat = np.array(line).reshape(M, N)\n",
    "\n",
    "    return mat\n",
    "\n",
    "def load_intrinsics(intrinsic_file, cam_id=0):\n",
    "    ''' load KITTIU360 perspective camera intrinsics \n",
    "    \n",
    "    Credit: https://github.com/autonomousvision/kitti360Scripts\n",
    "    '''\n",
    "\n",
    "    intrinsic_loaded = False\n",
    "    width = -1\n",
    "    height = -1\n",
    "    with open(intrinsic_file) as f:\n",
    "        intrinsics = f.read().splitlines()\n",
    "    for line in intrinsics:\n",
    "        line = line.split(' ')\n",
    "        if line[0] == f'P_rect_0{cam_id}:':\n",
    "            K = [float(x) for x in line[1:]]\n",
    "            K = np.reshape(K, [3,4])\n",
    "            intrinsic_loaded = True\n",
    "        elif line[0] == f'R_rect_0{cam_id}:':\n",
    "            R_rect = np.eye(4) \n",
    "            R_rect[:3,:3] = np.array([float(x) for x in line[1:]]).reshape(3,3)\n",
    "        elif line[0] == f\"S_rect_0{cam_id}:\":\n",
    "            width = int(float(line[1]))\n",
    "            height = int(float(line[2]))\n",
    "    assert(intrinsic_loaded==True)\n",
    "    assert(width>0 and height>0)\n",
    "    \n",
    "    return K, R_rect, width, height\n",
    "        \n",
    "        \n",
    "def loadCalibrationCameraToPose(filename):\n",
    "    ''' load KITTIU360 camera-to-pose calibration \n",
    "    \n",
    "    Credit: https://github.com/autonomousvision/kitti360Scripts\n",
    "    '''\n",
    "    Tr = {}\n",
    "    with open(filename,'r') as fid:\n",
    "        cameras = ['image_00', 'image_01', 'image_02', 'image_03']\n",
    "        lastrow = np.array([0,0,0,1]).reshape(1,4)\n",
    "        for camera in cameras:\n",
    "            Tr[camera] = np.concatenate((readVariable(fid, camera, 3, 4), lastrow))\n",
    "    return Tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '/raid/dataset/pointcloud/data/kitti_360/KITTI-360'\n",
    "raw_3d_dir = osp.join(DATA_ROOT, 'data_3d_semantics')\n",
    "raw_2d_dir = osp.join(DATA_ROOT, 'data_2d_raw')\n",
    "camera_names = ['image_00', 'image_01']\n",
    "\n",
    "scan_names = sorted(os.listdir(raw_3d_dir))\n",
    "\n",
    "seq = scan_names[0]\n",
    "cam_id = 0\n",
    "\n",
    "# Initialize file paths\n",
    "pose_dir = osp.join(DATA_ROOT, 'data_poses', seq)\n",
    "calib_dir = osp.join(DATA_ROOT, 'calibration')\n",
    "intrinsic_file = osp.join(calib_dir, 'perspective.txt')\n",
    "pose_file = osp.join(pose_dir, 'poses.txt')\n",
    "fileCameraToPose = osp.join(calib_dir, 'calib_cam_to_pose.txt')\n",
    "\n",
    "# Camera-to-pose calibration\n",
    "camToPose = torch.from_numpy(loadCalibrationCameraToPose(fileCameraToPose)[f'image_{cam_id:02d}'])\n",
    "\n",
    "# System poses (different from camera pose)\n",
    "poses = np.loadtxt(pose_file)[:350:20]\n",
    "frames = poses[:, 0].astype(np.int64)\n",
    "poses = torch.from_numpy(poses[:, 1:]).view(-1, 3, 4)\n",
    "\n",
    "n_images = poses.shape[0]\n",
    "\n",
    "# Intrinsic parameters\n",
    "K, R_rect, width, height = load_intrinsics(intrinsic_file, cam_id=cam_id)\n",
    "fx = torch.Tensor([K[0, 0]]).repeat(n_images)\n",
    "fy = torch.Tensor([K[1, 1]]).repeat(n_images)\n",
    "mx = torch.Tensor([K[0, 2]]).repeat(n_images)\n",
    "my = torch.Tensor([K[1, 2]]).repeat(n_images)\n",
    "R_rect = torch.from_numpy(R_rect)\n",
    "\n",
    "# Recover the cam2world from system pose and calibration\n",
    "cam2world = {}\n",
    "for frame, pose in zip(frames, poses): \n",
    "    \n",
    "    pose = torch.cat((pose, torch.ones(1, 4)), dim=0)\n",
    "    # consider the rectification for perspective cameras\n",
    "    if cam_id==0:\n",
    "        cam2world[frame] = pose @ camToPose @ torch.inverse(R_rect)\n",
    "#         cam2world[frame] = np.matmul(np.matmul(pose, self.camToPose), np.linalg.inv(R_rect))\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unknown Camera ID '{cam_id}'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_frame = 15\n",
    "T = cam2world[frames[i_frame]][:3, 3]\n",
    "R = cam2world[frames[i_frame]][:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.data_transform.grid_transform import GridSampling3D\n",
    "\n",
    "# voxel = 0.05\n",
    "voxel = 0.1\n",
    "    \n",
    "scan_ply_chunks = sorted(os.listdir(osp.join(raw_3d_dir, scan_names[0], 'static')))\n",
    "data = read_chunk(osp.join(raw_3d_dir, scan_names[0], 'static', scan_ply_chunks[0]))\n",
    "\n",
    "data = GridSampling3D(voxel, mode='last')(data)\n",
    "del data.grid_size\n",
    "\n",
    "radius = 50\n",
    "in_range = (data.pos - T).norm(dim=1) < radius\n",
    "data = Data(**{x: data[x][in_range] for x in data.keys})\n",
    "data.mapping_index = torch.arange(data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.multimodal.visibility import pinhole_projection_cuda, field_of_view_cuda\n",
    "\n",
    "u, v, z = pinhole_projection_cuda(data.pos, cam2world[frames[i_frame]], K, camera='kitti360_perspective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_fov = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "in_fov[field_of_view_cuda(u, v, x_min=0, x_max=width, y_min=0, y_max=height, z=z, img_mask=None)] = True\n",
    "\n",
    "alpha = 0.6\n",
    "data.rgb[in_fov] = data.rgb[in_fov] * alpha + torch.Tensor([1, 0, 0]) * (1 - alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = np.array([osp.join(raw_2d_dir, scan_names[0], camera_names[0], 'data_rect', f'{frames[i_frame]:010d}.png')])\n",
    "image_data = SameSettingImageData(ref_size=(width, height), proj_upscale=1, path=image_path, pos=T.unsqueeze(0), fx=fx[0].unsqueeze(0), fy=fy[0].unsqueeze(0), mx=mx[0].unsqueeze(0), my=my[0].unsqueeze(0), extrinsic=cam2world[frames[i_frame]].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data = MMData(data=data, image=ImageData(image_data))\n",
    "\n",
    "visualize_mm_data(mm_data, figsize=1000, pointsize=3, voxel=voxel, show_2d=True, class_names=class_names, class_colors=class_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.data_transform.multimodal.image import MapImages\n",
    "\n",
    "transform = MapImages(method='SplattingVisibility', ref_size=(width, height), proj_upscale=1, use_cuda=True, voxel=voxel, r_max=radius, r_min=0, k_swell=1.5, d_swell=10**6, exact=False, camera='kitti360_perspective', verbose=True)\n",
    "out = transform(data, image_data)\n",
    "mm_data = MMData(data=out[0], image=ImageData(out[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mm_data(mm_data, figsize=1000, pointsize=3, voxel=voxel, show_2d=True, class_names=class_names, class_colors=class_colors, front='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open(osp.join(raw_2d_dir, scan_names[0], camera_names[0], 'data_rect', f'{frames[i_frame]:010d}.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:depth_maps]",
   "language": "python",
   "name": "conda-env-depth_maps-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
