{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic multimodal data\n",
    "This notebook generates synthetic indoor point clouds and S3DIS-like images to experiment with multimodal mapping and visualize results. This can be helpful if you want to visualize how the mapping works, the impact of image transforms and how 3D and 2D subsampling affect the mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select you GPU\n",
    "I_GPU = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use autoreload\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "torch.cuda.set_device(I_GPU)\n",
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "\n",
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data samples\n",
    "\n",
    "Generates room-like boxes with different colors for walls, ceilings and floor, to facilitate subsequent visualizations. Not that you can tune this to 3D spaces of your taste and change the voxel resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.multimodal import *\n",
    "from torch_points3d.core.data_transform.transforms import RandomNoise\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "n_data = 1        # number of \"rooms\" to generate\n",
    "n_points = 10**5  # number of points per \"room\"\n",
    "height = 2.5      # floor-to-ceiling height\n",
    "width = 4         # width of the \"room\"\n",
    "\n",
    "data_list = []\n",
    "for i in range(n_data):\n",
    "    \n",
    "    # Offset\n",
    "    offset = torch.Tensor([[width * 2 * i, width * 2 * i, 0]])\n",
    "    \n",
    "    # XYZ\n",
    "    floor = torch.cat([torch.rand(n_points, 1) * width, torch.rand(n_points, 1) * width, torch.zeros(n_points, 1)], dim=1)\n",
    "    ceiling = torch.cat([torch.rand(n_points, 1) * width, torch.rand(n_points, 1) * width, torch.ones(n_points, 1) * height], dim=1)\n",
    "    wall_1 = torch.cat([torch.rand(n_points, 1) * width, torch.zeros(n_points, 1), torch.rand(n_points, 1) * height], dim=1)\n",
    "    wall_2 = torch.cat([torch.rand(n_points, 1) * width, torch.ones(n_points, 1) * width, torch.rand(n_points, 1) * height], dim=1)\n",
    "    wall_3 = torch.cat([torch.zeros(n_points, 1), torch.rand(n_points, 1) * width, torch.rand(n_points, 1) * height], dim=1)\n",
    "    wall_4 = torch.cat([torch.ones(n_points, 1) * width, torch.rand(n_points, 1) * width, torch.rand(n_points, 1) * height], dim=1)\n",
    "    xyz = torch.cat([floor, ceiling, wall_1, wall_2, wall_3, wall_4], dim=0) + offset\n",
    "\n",
    "    # RGB\n",
    "    floor = torch.Tensor([1, 0, 0]).view(1, -1).repeat(n_points, 1)    # red\n",
    "    ceiling = torch.Tensor([0, 1, 0]).view(1, -1).repeat(n_points, 1)  # green\n",
    "    wall_1 = torch.Tensor([0, 0, 1]).view(1, -1).repeat(n_points, 1)   # blue\n",
    "    wall_2 = torch.Tensor([1, 1, 0]).view(1, -1).repeat(n_points, 1)   # \n",
    "    wall_3 = torch.Tensor([0, 1, 1]).view(1, -1).repeat(n_points, 1)   # \n",
    "    wall_4 = torch.Tensor([1, 0, 1]).view(1, -1).repeat(n_points, 1)   # \n",
    "    rgb = torch.cat([floor, ceiling, wall_1, wall_2, wall_3, wall_4], dim=0)\n",
    "    \n",
    "    # Y\n",
    "    y = torch.repeat_interleave(torch.arange(6, dtype=torch.long), n_points)\n",
    "    \n",
    "    # Add noise to the data\n",
    "    data = RandomNoise(sigma=0.001, clip=0.05)(Data(pos=xyz, rgb=rgb, y=y))\n",
    "    \n",
    "    # Convert to Data object\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess 3D Data\n",
    "\n",
    "Basic preprocessings required to prepare mapping and mapping features computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.data_transform import *\n",
    "\n",
    "voxel = 0.05  # voxel grid resolution\n",
    "\n",
    "data_list = GridSampling3D(size=voxel, setattr_full_pos=True, quantize_coords=True, mode='last')(data_list)\n",
    "data_list = SaveOriginalPosId(key='mapping_index')(data_list)\n",
    "data_list = PCAComputePointwise(num_neighbors=50, r=2*voxel, use_full_pos=True)(data_list)\n",
    "data_list = EigenFeatures(norm=True, linearity=True, planarity=True, scattering=True, temperature=None)(data_list)\n",
    "data_list = RemoveAttributes(attr_names=['full_pos', 'eigenvalues', 'eigenvectors'])(data_list)\n",
    "data_list = AddFeatsByKeys(list_add_to_x=[True] * 4, feat_names=['norm', 'linearity', 'planarity', 'scattering'], delete_feats=[True] * 4)(data_list)\n",
    "# data_list = AddFeatsByKeys(list_add_to_x=[False, False, True, False], feat_names=['norm', 'linearity', 'planarity', 'scattering'], delete_feats=[False] * 4)(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic ImageData\n",
    "\n",
    "Generate random camera poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_size = (512, 256)  # pixel resolution of the images\n",
    "n_images_in = 4        # number of images inside of each \"room\"\n",
    "n_images_out = 4       # number of images outside of each \"room\"\n",
    "n_images = n_images_in + n_images_out\n",
    "\n",
    "image_data_list = []\n",
    "for i in range(n_data):\n",
    "    \n",
    "    # Recover the offset of the cloud\n",
    "    offset = data_list[i].pos.min(dim=0).values\n",
    "    \n",
    "    # Images inside of the rooms\n",
    "    xyz_in = torch.cat([\n",
    "        torch.rand(n_images_in, 1) * width * 4 / 5 + width / 10,\n",
    "        torch.rand(n_images_in, 1) * width * 4 / 5 + width / 5,\n",
    "        torch.rand(n_images_in, 1) * height * 2 / 3 + height / 6], dim=1)\n",
    "    xyz_in += offset\n",
    "\n",
    "    # Images outside of the rooms\n",
    "    radius = 1.5 * width + torch.rand(n_images_out)\n",
    "    theta = torch.rand(n_images_out) * 2 * np.pi\n",
    "    z = torch.rand(n_images_out, 1) * height * 3 / 2\n",
    "    xyz_out = offset.view(1, -1) + torch.cat([\n",
    "        (torch.cos(theta) * radius).view(-1, 1),\n",
    "        (torch.sin(theta) * radius).view(-1, 1),\n",
    "        z], dim=1)\n",
    "\n",
    "    # Convert to SameSettingImageData\n",
    "    path = np.array([''] * n_images)\n",
    "    pos = torch.cat([xyz_in, xyz_out], dim=0)\n",
    "    opk = torch.zeros(n_images, 3)\n",
    "    image_data_list.append(SameSettingImageData(\n",
    "        path=path, pos=pos, opk=opk, ref_size=ref_size, proj_upscale=2, \n",
    "        downscale=1, voxels=voxel, r_max=30, r_min=0.3, growth_k=0.2,\n",
    "        growth_r=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3D Data onto ImageData to generate mappings\n",
    "\n",
    "This is where the mapping and associated features are actually computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.data_transform.multimodal.image import *\n",
    "\n",
    "r_max = 10                        # maximum point-camera distance for the mappings\n",
    "r_min = 0.2                       # minimum point-camera distance for the mappings \n",
    "k_list = [50]                     # number of neighbors used for neighborhood-based mapping features (eg density, occlusion)\n",
    "exact = True                      # False: points are mapped to their whole z-buffering patch (denser mapping). True: only to the center (more accurate mapping)  \n",
    "use_cuda = True                   # whether to use cuda to accelerate mapping computation\n",
    "camera = 's3dis_equirectangular'  # camera model used (keep s3dis_equirectangular for this notebook)\n",
    "\n",
    "data_list, image_data_list = MapImages(r_min=r_min, r_max=r_max, exact=exact, use_cuda=use_cuda, camera=camera)(data_list, image_data_list)\n",
    "# image_data_list[0].mappings.features = None  # uncomment to visualize only densities and occlusions\n",
    "data_list, image_data_list = NeighborhoodBasedMappingFeatures(k=k_list, voxel=voxel, density=True, occlusion=True, use_cuda=True)(data_list, image_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate the image features with 3D point's RGB\n",
    "\n",
    "Create synthetic views of the point cloud from the generated camera poses, by z-buffering the voxel splats. In other words, the mapping is used to propagate the points' RGB colors to pixels and thus create a rendering of the 3D scene for each pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, images in zip(data_list, image_data_list):\n",
    "    \n",
    "    # Initialize backgrounds to black\n",
    "    x = torch.zeros(n_images, 3, images.img_size[1], images.img_size[0]).byte()\n",
    "    images.x = x\n",
    "        \n",
    "    for i, im in enumerate(images):\n",
    "        # Get the mapping of all points in the sample\n",
    "        idx = im.mappings.feature_map_indexing\n",
    "        idx = (torch.ones_like(idx[0]) * i, *idx[1:])\n",
    "    \n",
    "        # Set mapping mask to point cloud RGB colors\n",
    "        color = (data.rgb * 255).type(torch.uint8)\n",
    "        color = torch.repeat_interleave(\n",
    "            color,\n",
    "            im.mappings.pointers[1:] - im.mappings.pointers[:-1],\n",
    "            dim=0)\n",
    "        color = torch.repeat_interleave(\n",
    "            color,\n",
    "            im.mappings.values[1].pointers[1:]\n",
    "            - im.mappings.values[1].pointers[:-1],\n",
    "            dim=0)\n",
    "        \n",
    "        # Apply the coloring to the mapping masks\n",
    "        images.x[idx] = color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the ImageData\n",
    "\n",
    "Apply some transforms to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list, image_data_list = RandomHorizontalFlip(p=0.5)(data_list, image_data_list)\n",
    "data_list, image_data_list = ToFloatImage()(data_list, image_data_list)\n",
    "data_list, image_data_list = CenterRoll(angular_res=16)(data_list, image_data_list)\n",
    "data_list, image_data_list = CropImageGroups(padding=2, min_size=32)(data_list, image_data_list)\n",
    "data_list, image_data_list = PickImagesFromMemoryCredit(img_size=ref_size, n_img=4, k_coverage=0)(data_list, image_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the MMData\n",
    "\n",
    "Check out the results in an interactive visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data\n",
    "\n",
    "OBJECT_COLOR = [\n",
    "    [95, 156, 196],  #'floor' .-> . blue\n",
    "    [233, 229, 107],  #'ceiling' .-> .yellow\n",
    "    [179, 116, 81],  #'wall_1'  ->  brown    \n",
    "    [108, 135, 75],  #'wall_2'   ->  dark green\n",
    "    [41, 49, 101],  #'wall_3'  ->  darkblue\n",
    "    [223, 52, 52],  #'wall_4'  ->  red\n",
    "    [0, 0, 0],  # unlabelled .->. black\n",
    "    ]\n",
    "\n",
    "CLASSES = [\n",
    "    'floor',\n",
    "    'ceiling',\n",
    "    'wall_1',\n",
    "    'wall_2',\n",
    "    'wall_3',\n",
    "    'wall_4',\n",
    "    'unlabelled',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data = MMBatch.from_mm_data_list([\n",
    "    MMData(data, image=images) \n",
    "    for data, images in zip(data_list, image_data_list)])\n",
    "\n",
    "visualize_mm_data(mm_data, class_names=CLASSES, class_colors=OBJECT_COLOR, figsize=1000, voxel=0.05, show_3d=True, show_2d=True, color_mode='light', alpha=5, pointsize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image features Multi-View Pooling visualization\n",
    "\n",
    "We propose to visualize here the impact of a simple mean-pool on the image features of each point's multiple views. As opposed to our DeepViewAgg, this approach does not learn to use the observation conditions (mapping features) to adress the multi-view problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.modules.multimodal.modules import UnimodalBranch\n",
    "from torch_points3d.modules.multimodal.fusion import BimodalFusion\n",
    "from torch_points3d.modules.multimodal.pooling import BimodalCSRPool\n",
    "\n",
    "branch = UnimodalBranch(None, BimodalCSRPool(mode='max'), BimodalCSRPool(mode='mean'), BimodalFusion(mode='residual'))\n",
    "mm_data_dict = {\n",
    "    'x_3d': torch.zeros_like(mm_data.data.rgb),\n",
    "    'x_seen': None,\n",
    "    'modalities': {'image': mm_data.modalities['image'].clone()}}\n",
    "mm_data_dict = branch.forward(mm_data_dict, 'image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data_out = mm_data.clone()\n",
    "mm_data_out.data.rgb = mm_data_dict['x_3d']\n",
    "\n",
    "visualize_mm_data(mm_data_out, class_names=CLASSES, class_colors=OBJECT_COLOR, figsize=1000, voxel=0.05, show_3d=True, show_2d=True, color_mode='light', alpha=5, pointsize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D downsampling visualization with TorchSparse module\n",
    "\n",
    "Visualize the impact of 3D downsampling on the mapping. Here the downsampling is performed using a 3D strided conv with kernel `downsampling_3d x downsampling_3d x downsampling_3d`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.modules.multimodal.modules import MultimodalBlockDown, UnimodalBranch\n",
    "from torch_points3d.modules.multimodal.fusion import BimodalFusion\n",
    "from torch_points3d.modules.multimodal.pooling import BimodalCSRPool\n",
    "from torch_points3d.modules.SparseConv3d.modules import ResNetDown\n",
    "from torch_points3d.modules.SparseConv3d.nn.torchsparse import SparseTensor, Conv3d\n",
    "\n",
    "# 3D resolution downscaling factor\n",
    "downsampling_3d = 8\n",
    "\n",
    "conv_3d = torch.nn.Sequential(\n",
    "    Conv3d(in_channels=3, out_channels=3, kernel_size=3, stride=1),\n",
    "    Conv3d(in_channels=3, out_channels=3, kernel_size=2, stride=downsampling_3d))\n",
    "branch = UnimodalBranch(\n",
    "    None,\n",
    "    BimodalCSRPool(mode='max'),\n",
    "    BimodalCSRPool(mode='mean'),\n",
    "    BimodalFusion(mode='residual'))\n",
    "mm_block = MultimodalBlockDown(conv_3d, None, image=branch).to('cuda')\n",
    "\n",
    "# Prepare data with the expected dictionary format\n",
    "mm_data_cuda = mm_data.clone().to('cuda')\n",
    "mm_data_dict = {\n",
    "    'x_3d': SparseTensor(\n",
    "        torch.zeros(mm_data.data.rgb.shape, device=mm_data_cuda.device),\n",
    "        mm_data_cuda.data.coords,\n",
    "        mm_data_cuda.data.batch,\n",
    "        mm_data_cuda.device),\n",
    "    'x_seen': None,\n",
    "    'modalities': {'image': mm_data_cuda.modalities['image'].clone()}}\n",
    "\n",
    "# Forward the 3D downsampling\n",
    "mm_data_dict_out = mm_block.forward(mm_data_dict)\n",
    "out_3d = mm_data_dict_out['x_3d']\n",
    "out_3d = Batch(\n",
    "    x=out_3d.F,\n",
    "    batch=out_3d.C[:, -1].long().to(out_3d.F.device),\n",
    "    pos=(out_3d.C[:, :3] * voxel).float().to(out_3d.F.device),\n",
    "    mapping_index=torch.arange(out_3d.F.shape[0]),\n",
    "    rgb=torch.zeros(out_3d.F.shape[0], 3, dtype=torch.uint8),\n",
    "    y=torch.zeros(out_3d.F.shape[0], dtype=torch.long))\n",
    "\n",
    "mm_data_out = MMData(out_3d, **mm_data_dict_out['modalities']).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mm_data(mm_data_out, class_names=CLASSES, class_colors=OBJECT_COLOR, figsize=1000, voxel=0.05, show_3d=True, show_2d=True, color_mode='light', alpha=5, pointsize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D downsampling visualization with image module\n",
    "\n",
    "Visualize the impact of 2D downsampling on the mapping. Here the downsampling is performed using a 2D strided conv with kernel `downsampling_2d x downsampling_2d`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.modules.multimodal.modules import UnimodalBranch\n",
    "from torch_points3d.modules.multimodal.fusion import BimodalFusion\n",
    "from torch_points3d.modules.multimodal.pooling import BimodalCSRPool\n",
    "from torch_points3d.modules.multimodal.modalities.image import ResNetDown\n",
    "\n",
    "# 2D resolution downscaling factor\n",
    "downsampling_2d = 8\n",
    "\n",
    "# Manually create a conv layer with averaging kernels\n",
    "class SomeDownConv(torch.nn.Module):\n",
    "    def __init__(self, stride, bias=False):\n",
    "        super(SomeDownConv, self).__init__()\n",
    "        conv_2d = torch.nn.Conv2d(3, 3, 2, stride=stride, bias=bias)\n",
    "        with torch.no_grad():\n",
    "            conv_2d.weight = torch.nn.Parameter(torch.zeros_like(conv_2d.weight))\n",
    "            for i in range(3):\n",
    "                conv_2d.weight[i, i] = 0.25\n",
    "        self.conv_2d = conv_2d\n",
    "    \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.conv_2d(x)\n",
    "\n",
    "branch = UnimodalBranch(\n",
    "    SomeDownConv(downsampling_2d),\n",
    "    BimodalCSRPool(mode='max'),\n",
    "    BimodalCSRPool(mode='mean'),\n",
    "    BimodalFusion(mode='residual'))\n",
    "\n",
    "# Prepare data with the expected dictionary format \n",
    "mm_data_dict = {\n",
    "    'x_3d': torch.zeros_like(mm_data.data.rgb),\n",
    "    'x_seen': None,\n",
    "    'modalities': {'image': mm_data.modalities['image'].clone()}}\n",
    "\n",
    "# Forward the 2D downsampling\n",
    "mm_data_dict_out = branch.forward(mm_data_dict, 'image')\n",
    "x_3d = mm_data_dict_out['x_3d']\n",
    "x_mod = mm_data_dict_out['modalities']['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data_out = mm_data.clone()\n",
    "mm_data_out.data.rgb = (x_3d - x_3d.min()) / (x_3d.max() + 1e-6)\n",
    "mm_data_out.modalities['image'] = x_mod\n",
    "\n",
    "visualize_mm_data(mm_data_out, class_names=CLASSES, class_colors=OBJECT_COLOR, figsize=1000, voxel=0.05, show_3d=True, show_2d=True, color_mode='light', alpha=5, pointsize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D-2D downsampling visualization in MultiModalBlock\n",
    "\n",
    "Visualize the impact of 2D and 3D subsampling at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.modules.multimodal.modules import MultimodalBlockDown, UnimodalBranch\n",
    "from torch_points3d.modules.multimodal.fusion import BimodalFusion\n",
    "from torch_points3d.modules.multimodal.pooling import BimodalCSRPool\n",
    "from torch_points3d.modules.SparseConv3d.modules import ResNetDown\n",
    "from torch_points3d.modules.SparseConv3d.nn.torchsparse import SparseTensor, Conv3d\n",
    "\n",
    "# 2D-3D resolution downscaling factor\n",
    "downsampling_2d = 4\n",
    "downsampling_3d = 4\n",
    "\n",
    "# Manually create a conv layer with averaging kernels\n",
    "class SomeDownConv(torch.nn.Module):\n",
    "    def __init__(self, stride, bias=False):\n",
    "        super(SomeDownConv, self).__init__()\n",
    "        conv_2d = torch.nn.Conv2d(3, 3, 2, stride=stride, bias=bias)\n",
    "        with torch.no_grad():\n",
    "            conv_2d.weight = torch.nn.Parameter(torch.zeros_like(conv_2d.weight))\n",
    "            for i in range(3):\n",
    "                conv_2d.weight[i, i] = 0.25\n",
    "        self.conv_2d = conv_2d\n",
    "    \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.conv_2d(x)\n",
    "\n",
    "branch = UnimodalBranch(\n",
    "    SomeDownConv(downsampling_2d),\n",
    "    BimodalCSRPool(mode='max'),\n",
    "    BimodalCSRPool(mode='mean'),\n",
    "    BimodalFusion(mode='residual'))\n",
    "\n",
    "conv_3d = torch.nn.Sequential(\n",
    "    Conv3d(in_channels=3, out_channels=3, kernel_size=3, stride=1),\n",
    "    Conv3d(in_channels=3, out_channels=3, kernel_size=2, stride=downsampling_3d))\n",
    "\n",
    "mm_block = MultimodalBlockDown(conv_3d, None, image=branch).to('cuda')\n",
    "\n",
    "# Prepare data with the expected dictionary format\n",
    "mm_data_cuda = mm_data.clone().to('cuda')\n",
    "mm_data_dict = {\n",
    "    'x_3d': SparseTensor(\n",
    "        torch.zeros(mm_data.data.rgb.shape, device=mm_data_cuda.device),\n",
    "        mm_data_cuda.data.coords,\n",
    "        mm_data_cuda.data.batch,\n",
    "        mm_data_cuda.device),\n",
    "    'x_seen': None,\n",
    "    'modalities': {'image': mm_data_cuda.modalities['image'].clone()}}\n",
    "\n",
    "# Forward the 3D downsampling\n",
    "mm_data_dict_out = mm_block.forward(mm_data_dict)\n",
    "out_3d = mm_data_dict_out['x_3d']\n",
    "out_3d = Batch(\n",
    "    x=out_3d.F,\n",
    "    batch=out_3d.C[:, -1].long().to(out_3d.F.device),\n",
    "    pos=(out_3d.C[:, :3] * voxel).float().to(out_3d.F.device),\n",
    "    mapping_index=torch.arange(out_3d.F.shape[0]),\n",
    "    rgb=torch.zeros(out_3d.F.shape[0], 3, dtype=torch.uint8),\n",
    "    y=torch.zeros(out_3d.F.shape[0], dtype=torch.long))\n",
    "\n",
    "mm_data_out = MMData(out_3d, **mm_data_dict_out['modalities']).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mm_data(mm_data_out, class_names=CLASSES, class_colors=OBJECT_COLOR, figsize=1000, voxel=0.05, show_3d=True, show_2d=True, color_mode='light', alpha=5, pointsize=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tp3d_dev] *",
   "language": "python",
   "name": "conda-env-tp3d_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
